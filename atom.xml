<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
    <id>https://bixiaopeng0.github.io</id>
    <title>晓鹏博客</title>
    <updated>2020-01-06T12:21:22.194Z</updated>
    <generator>https://github.com/jpmonette/feed</generator>
    <link rel="alternate" href="https://bixiaopeng0.github.io"/>
    <link rel="self" href="https://bixiaopeng0.github.io/atom.xml"/>
    <subtitle>温故而知新</subtitle>
    <logo>https://bixiaopeng0.github.io/images/avatar.png</logo>
    <icon>https://bixiaopeng0.github.io/favicon.ico</icon>
    <rights>All rights reserved 2020, 晓鹏博客</rights>
    <entry>
        <title type="html"><![CDATA[配置opencv dnn（cuda）以及openvino遇到的坑]]></title>
        <id>https://bixiaopeng0.github.io/post/pei-zhi-opencv-dnncudayi-ji-openvino-yu-dao-de-keng</id>
        <link href="https://bixiaopeng0.github.io/post/pei-zhi-opencv-dnncudayi-ji-openvino-yu-dao-de-keng">
        </link>
        <updated>2020-01-06T12:15:48.000Z</updated>
        <content type="html"><![CDATA[<!--more-->!
<p>配置openvino遇到的坑</p>
<p>1、第一次下载的R1,0但是检测不到python,后来下载R3.1解决问题<br>
2、编译build_saples_msvc报错<br>
将一下内容设置为<br>
set MSBUILD_BIN=C:\Program Files (x86)\MSBuild\14.0\Bin\MSBuild.exe<br>
set VS_PATH=<br>
set VS_VERSION=2015<br>
添加<br>
set MSBUILD_VERSION=14 2015<br>
set PLATFORM=x64</p>
<p>3、opencv编译报错<br>
重新编译opencv 添加IE库</p>
<p>4、cmake情况下会出现下载失败，先从日记里面把安装包下载好，然后在进行config</p>
<p>配置dnn——cuda</p>
<p>1、 使用环境</p>
<p>（1）Cuda10.0</p>
<p>（2）Cudnn7.5及以上（和cuda版本兼容）</p>
<p>（3）Vs版本：2017（2015经测试，编译失败）</p>
<p>（4）Cmake</p>
<p>2、 配置步骤</p>
<p>（1） 选择源码路径，然后config，见图一</p>
<p>（2） 勾选build_opencv_world, opencv_dnn_cuda with_cuda,将解压好的opencv_contribe的module路径添加进来，注contribe可以在github上下载最新的，见图二,三,四</p>
<p>（3） 第二次config，部分文件可能会下载失败，可以手动下载，放到指定文件夹下即可，从nvidia官网查找本机显卡算力，然后在cuda_arch_bin中保留本机显卡算力，删除其他，然后勾选cuda_fast_math,第三次configure  见图五</p>
<p>（4） Configure完成之后，从日记查看是否检测到本机的cuda和cudnn，如果检测到点击generate，见图六</p>
<p>（5） 打开vs工程，在release模型下点击ALL_BUILD，生成之后，右击INSTALL-&gt;仅用于项目-&gt;仅生成INSTALL</p>
<p>（6） 配置好opencv的环境，在代码里使用指定参数，即可调用cuda 见图七</p>
<p>图片是懒得传上图床了。。。。</p>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[opencv dnn模块]]></title>
        <id>https://bixiaopeng0.github.io/post/opencv-dnn-mo-kuai</id>
        <link href="https://bixiaopeng0.github.io/post/opencv-dnn-mo-kuai">
        </link>
        <updated>2019-10-12T13:15:22.000Z</updated>
        <content type="html"><![CDATA[<!--more-->
<p>最近在研究opencv的dnn模块，简单的做一个记录，以备后面使用。</p>
<h3 id="dnn简介">dnn简介</h3>
<p>​        dnn是opencv中的深度神经网络模块，支持运行网络，但是不支持训练，实现图像场景中的图像分类，图像检测和图像分割，并且支持主流的深度学习框架。</p>
<h3 id="dnn尝试">dnn尝试</h3>
<p>​       最近一端时间尝试了tensorflow,darknet还有caffe。调通了darknet的yolov3还有caffe的ssd_mobilnet检测网络，但是都是基于cpu的，所以速度可想而知，yolov3速度大约在1~2fps，ssd-mobilnet大约在9fps。</p>
<p>​      googlenet模型是从caffe官方github仓库里面下的，从网上下载了一张雨伞图片，测试还可可以，准确率蛮高的。</p>
<h3 id="资料来源">资料来源</h3>
<p>​      代码主要是在github上，还有各个博客上找的。其实opencv也提供了相关demo，网上的代码也是基于demo改的，安装完opencv之后，在source/samples/dnn里面提供了相关代码，可以参考。</p>
<h3 id="问题记录">问题记录</h3>
<ul>
<li>首先是tensorflow下的ssd网络，如果调用opencv里面的权重是可以跑通的，但是自己生成的却不行，但是网上有大佬用自己训练的模型跑通了。</li>
<li>使用opencl加速，代码里有一个参数可以进行加速，但是使用之后就报错，从网上查了一下
<ul>
<li>opencv对显卡支持不好 -- github opencv issue</li>
<li>opencv只支持intel显卡-- 这个是前几年的文章看见的，现在支持了。</li>
</ul>
</li>
</ul>
<h3 id="总结">总结</h3>
<ul>
<li>个人感觉darknet是用c写的，caffe用c++写的，比较容易调通，调用过程基本上没有遇到困难，但是tensorflow和pytorch使用python写的，调用却不太顺利，是不是和语言有关系呢？</li>
<li>dnn模块软肋
<ul>
<li>只支持部分网络</li>
<li>gpu加速还是个问题，可能还不太成熟吧，博客上看见的也是用cpu调通的，但是速度这么慢，落地不行呀。</li>
</ul>
</li>
</ul>
<pre><code class="language-c++">##caffe-ssd mobilnet opencv3.4.1

#include&lt;iostream&gt;
#include&lt;opencv2/opencv.hpp&gt;
#include&lt;opencv2/dnn.hpp&gt;
using namespace std;
using namespace cv;
using namespace cv::dnn;
class Object
{
public:
	Object();
	Object(int index, float confidence, String name, Rect rect);
	~Object();
public:
	int index;
	String name;
	float confidence;
	Rect rect;
private:
};
Object::Object() {
}
Object::Object(int index, float confidence, String name, Rect rect) {
	this-&gt;index = index;
	this-&gt;confidence = confidence;
	this-&gt;name = name;
	this-&gt;rect = rect;
}
Object::~Object() {
}
//----------------------------全局常量----------------------------------
//配置好protxt文件，网络结构描述文件
//配置好caffemodel文件，训练好的网络权重
const String PROTOTX_FILE = &quot;no_bn.prototxt&quot;;
const String CAFFE_MODEL_FILE = &quot;no_bn.caffemodel&quot;;
const String classNames[] = { &quot;background&quot;, &quot;jingling&quot;, &quot;t16&quot;, &quot;xiao_spark&quot;, &quot;yu_mavic&quot;};
const float CONF_THRESH = 0.5;
int main() {
	//------------------------实例化网络----------------------------
	Net mobileNetSSD = readNetFromCaffe(PROTOTX_FILE, CAFFE_MODEL_FILE);
	if (mobileNetSSD.empty()) {
		cerr &lt;&lt; &quot;加载网络失败！&quot; &lt;&lt; endl;
		return -1;
	}
	TickMeter t;
	VideoCapture video;
	video.open(&quot;scr.mp4&quot;);
	
	Mat srcImg;
	video &gt;&gt; srcImg;
	while (!srcImg.empty())
	{
		clock_t start_time,end_time;
		start_time = clock();
		//----------------------设置网络输入-----------------------
		//将二维图像转换为CNN输入的张量Tensor,作为网络的输入
		mobileNetSSD.setInput(blobFromImage(srcImg, 1.0 / 127.5, Size(300, 300), Scalar(127.5, 127.5, 127.5), true, false));
		t.start();
		//--------------------CNN网络前向计算----------------------
		Mat netOut = mobileNetSSD.forward();
		t.stop();
		//----------------------解析计算结果-----------------------
		vector&lt;Object&gt; detectObjects;
		Mat detectionResult(netOut.size[2], netOut.size[3], CV_32F, netOut.ptr&lt;float&gt;());
		for (int i = 0; i &lt; detectionResult.rows; i++) {
			//目标类别的索引
			int objectIndex = detectionResult.at&lt;float&gt;(i, 1);
			//检测结果置信度
			float confidence = detectionResult.at&lt;float&gt;(i, 2);
			//根据置信度阈值过滤掉置信度较小的目标
			if (confidence&lt;CONF_THRESH) {
				continue;
			}
			//反归一化，得到图像坐标
			int xLeftUp = static_cast&lt;int&gt;(detectionResult.at&lt;float&gt;(i, 3)*srcImg.cols);
			int yLeftUp = static_cast&lt;int&gt;(detectionResult.at&lt;float&gt;(i, 4)*srcImg.rows);
			int xRightBottom = static_cast&lt;int&gt;(detectionResult.at&lt;float&gt;(i, 5)*srcImg.cols);
			int yRightBottom = static_cast&lt;int&gt;(detectionResult.at&lt;float&gt;(i, 6)*srcImg.rows);
			//矩形框
			Rect rect(Point{ xLeftUp,yLeftUp }, Point{ xRightBottom,yRightBottom });
			//保存结果
			detectObjects.push_back(Object{ objectIndex,confidence,classNames[objectIndex],rect });
		}
		//------------------------显示结果-----------------------------------
		int count = 0;
		for (auto&amp; i : detectObjects) {
			rectangle(srcImg, i.rect, Scalar(0, 255, 255), 2);
			putText(srcImg, i.name, i.rect.tl(), 1, 1.8, Scalar(255, 0, 0), 2);
			cout &lt;&lt; &quot;第&quot; &lt;&lt; count &lt;&lt; &quot;个目标：&quot; &lt;&lt; i.name &lt;&lt; &quot;\t&quot; &lt;&lt; i.rect &lt;&lt; &quot;\t&quot; &lt;&lt; i.confidence &lt;&lt; endl;
			count++;
		}


		end_time = clock();
		imshow(&quot;MobileNet-SSD&quot;, srcImg);
		waitKey(10);
		cout &lt;&lt; &quot;FPS:&quot; &lt;&lt; CLOCKS_PER_SEC/(double)(end_time - start_time)  &lt;&lt; endl;
		video &gt;&gt; srcImg;
	}


	waitKey(0);
}

</code></pre>
<pre><code class="language-c++">### opencv4.1 darknet-yolov3
#include &lt;opencv2/opencv.hpp&gt;
#include &lt;opencv2/dnn.hpp&gt;

#include &lt;fstream&gt;
#include &lt;iostream&gt;
#include &lt;algorithm&gt;
#include &lt;cstdlib&gt;
using namespace std;
using namespace cv;
using namespace cv::dnn;
void image_detection();
void video_detection();
float confidenceThreshold = 0.25;
String yolo_cfg = &quot;yolov3-voc.cfg&quot;;
String yolo_model = &quot;yolov3-voc_f.weights&quot;;

int main(int argc, char** argv)
{
	//image_detection();
	video_detection();
}

void image_detection() {
	Net net = readNetFromDarknet(yolo_cfg, yolo_model);
	net.setPreferableBackend(DNN_BACKEND_OPENCV);
	net.setPreferableTarget(DNN_TARGET_CPU);
	//net.setPreferableBackend(DNN_BACKEND_OPENCV);       //启动GPU加速
	//net.setPreferableTarget(DNN_TARGET_OPENCL);         //启动GPU加速
	std::vector&lt;String&gt; outNames = net.getUnconnectedOutLayersNames();
	for (int i = 0; i &lt; outNames.size(); i++) {
		printf(&quot;output layer name : %s\n&quot;, outNames[i].c_str());
	}


	vector&lt;string&gt; classNamesVec;
	//ifstream classNamesFile(&quot;object_detection_classes_yolov3.txt&quot;);
	ifstream classNamesFile(&quot;myData.names&quot;);
	if (classNamesFile.is_open())
	{
		string className = &quot;&quot;;
		while (std::getline(classNamesFile, className))
			classNamesVec.push_back(className);

	}
	clock_t start_time;
	start_time = clock();
	// 加载图像 
	Mat frame = imread(&quot;scr.jpg&quot;);
	//Mat frame = imread(&quot;cat.jpg&quot;);
	Mat inputBlob = blobFromImage(frame, 1 / 255.F, Size(416, 416), Scalar(), true, false);
	net.setInput(inputBlob);

	// 检测
	std::vector&lt;Mat&gt; outs;
	net.forward(outs, outNames);
	vector&lt;double&gt; layersTimings;
	double freq = getTickFrequency() / 1000;
	double time = net.getPerfProfile(layersTimings) / freq;
	ostringstream ss;
	cout &lt;&lt; &quot;detection time: &quot; &lt;&lt; time &lt;&lt; &quot; ms&quot;;
	//putText(frame, ss.str(), Point(20, 20), 0, 0.5, Scalar(0, 0, 255));
	vector&lt;Rect&gt; boxes;
	vector&lt;int&gt; classIds;
	vector&lt;float&gt; confidences;
	for (size_t i = 0; i&lt;outs.size(); ++i)
	{
		// Network produces output blob with a shape NxC where N is a number of
		// detected objects and C is a number of classes + 4 where the first 4
		// numbers are [center_x, center_y, width, height]
		float* data = (float*)outs[i].data;
		for (int j = 0; j &lt; outs[i].rows; ++j, data += outs[i].cols)
		{
			Mat scores = outs[i].row(j).colRange(5, outs[i].cols);
			Point classIdPoint;
			double confidence;
			minMaxLoc(scores, 0, &amp;confidence, 0, &amp;classIdPoint);
			if (confidence &gt; confidenceThreshold)
			{
				int centerX = (int)(data[0] * frame.cols);
				int centerY = (int)(data[1] * frame.rows);
				int width = (int)(data[2] * frame.cols);
				int height = (int)(data[3] * frame.rows);
				int left = centerX - width / 2;
				int top = centerY - height / 2;

				classIds.push_back(classIdPoint.x);
				confidences.push_back((float)confidence);
				boxes.push_back(Rect(left, top, width, height));
			}
		}
	}

	vector&lt;int&gt; indices;
	NMSBoxes(boxes, confidences, 0.5, 0.2, indices);
	for (size_t i = 0; i &lt; indices.size(); ++i)
	{
		int idx = indices[i];
		Rect box = boxes[idx];
		String className = classNamesVec[classIds[idx]] ;
		putText(frame, className.c_str(), box.tl(), FONT_HERSHEY_SIMPLEX, 1, Scalar(255, 0, 0), 2, 8);
		rectangle(frame, box, Scalar(0, 0, 255), 2, 8, 0);
	}
	cout &lt;&lt;&quot;运行时间：&quot; &lt;&lt; ((double)(clock() - start_time) / CLOCKS_PER_SEC) * 1000 &lt;&lt; &quot;ms&quot; &lt;&lt; &quot;\n&quot; &lt;&lt; endl;
	imshow(&quot;YOLOv3-Detections&quot;, frame);
	waitKey(0);
	return;
}

void video_detection() {

	Net net = readNetFromDarknet(yolo_cfg, yolo_model);
	net.setPreferableBackend(DNN_BACKEND_OPENCV);
	net.setPreferableTarget(DNN_TARGET_CPU);
	//net.setPreferableBackend(DNN_BACKEND_OPENCV);       //启动GPU加速
	//net.setPreferableTarget(DNN_TARGET_OPENCL);         //启动GPU加速
	if (net.empty())
	{
		printf(&quot;Could not load net...\n&quot;);
		return;
	}
	std::vector&lt;String&gt; outNames = net.getUnconnectedOutLayersNames();
	for (int i = 0; i &lt; outNames.size(); i++) {
		printf(&quot;output layer name : %s\n&quot;, outNames[i].c_str());
	}
	vector&lt;string&gt; classNamesVec;
	ifstream classNamesFile(&quot;myData.names&quot;);
	if (classNamesFile.is_open())
	{
		string className = &quot;&quot;;
		while (std::getline(classNamesFile, className))
			classNamesVec.push_back(className);
	}

	VideoCapture video_read;
	video_read.open(&quot;scr2.mp4&quot;);
	Mat frame;
	video_read &gt;&gt; frame;
	/*
	VideoCapture capture;
	capture.open(&quot;D:/vcprojects/images/fbb.avi&quot;);
	if (!capture.isOpened()) {
		printf(&quot;could not open the camera...\n&quot;);
		return;
	}
	*/

	while (!frame.empty())
	{
		//加载图像
		//if (frame.empty())
		//	if (frame.channels() == 4)
		//		cvtColor(frame, frame, COLOR_BGRA2BGR);
		Mat inputBlob = blobFromImage(frame, 1 / 255.F, Size(416, 416), Scalar(), true, false);
		net.setInput(inputBlob, &quot;data&quot;);
		
	


		//Mat detectionMat = net.forward(&quot;detection_out&quot;);
		//vector&lt;double&gt; layersTimings;
		//double freq = getTickFrequency() / 1000;
		//double time = net.getPerfProfile(layersTimings) / freq;
		//ostringstream ss;
		//ss &lt;&lt; &quot;FPS: &quot; &lt;&lt; 1000 / time &lt;&lt; &quot; ; time: &quot; &lt;&lt; time &lt;&lt; &quot; ms&quot;;
		//putText(frame, ss.str(), Point(20, 20), 0, 0.5, Scalar(0, 0, 255));


		// 检测
		std::vector&lt;Mat&gt; outs;
		net.forward(outs, outNames);
		vector&lt;double&gt; layersTimings;
		double freq = getTickFrequency() / 1000;
		double time = net.getPerfProfile(layersTimings) / freq;
		ostringstream ss;
		ss &lt;&lt; &quot;detection time: &quot; &lt;&lt; time &lt;&lt; &quot; ms&quot;;
		putText(frame, ss.str(), Point(20, 20), 0, 0.5, Scalar(0, 0, 255));
		vector&lt;Rect&gt; boxes;
		vector&lt;int&gt; classIds;
		vector&lt;float&gt; confidences;

		for (size_t i = 0; i &lt; outs.size(); ++i)
		{
			// Network produces output blob with a shape NxC where N is a number of
			// detected objects and C is a number of classes + 4 where the first 4
			// numbers are [center_x, center_y, width, height]
			float* data = (float*)outs[i].data;
			for (int j = 0; j &lt; outs[i].rows; ++j, data += outs[i].cols)
			{
				Mat scores = outs[i].row(j).colRange(5, outs[i].cols);
				Point classIdPoint;
				double confidence;
				minMaxLoc(scores, 0, &amp;confidence, 0, &amp;classIdPoint);
				if (confidence &gt; confidenceThreshold)
				{
					int centerX = (int)(data[0] * frame.cols);
					int centerY = (int)(data[1] * frame.rows);
					int width = (int)(data[2] * frame.cols);
					int height = (int)(data[3] * frame.rows);
					int left = centerX - width / 2;
					int top = centerY - height / 2;

					classIds.push_back(classIdPoint.x);
					confidences.push_back((float)confidence);
					boxes.push_back(Rect(left, top, width, height));
				}
			}
		}

		vector&lt;int&gt; indices;
		NMSBoxes(boxes, confidences, 0.5, 0.2, indices);
		for (size_t i = 0; i &lt; indices.size(); ++i)
		{
			int idx = indices[i];
			Rect box = boxes[idx];
			String className = classNamesVec[classIds[idx]] + std::to_string(confidences[idx]);
			putText(frame, className.c_str(), box.tl(), FONT_HERSHEY_SIMPLEX, 1, Scalar(255, 0, 0), 2, 8);
			rectangle(frame, box, Scalar(0, 0, 255), 2, 8, 0);
		}

		/*for (int i = 0; i &lt; detectionMat.rows; i++)
		{
			const int probability_index = 5;
			const int probability_size = detectionMat.cols - probability_index;
			float *prob_array_ptr = &amp;detectionMat.at&lt;float&gt;(i, probability_index);
			size_t objectClass = max_element(prob_array_ptr, prob_array_ptr + probability_size) - prob_array_ptr;
			float confidence = detectionMat.at&lt;float&gt;(i, (int)objectClass + probability_index);
			if (confidence &gt; confidenceThreshold)
			{
				float x = detectionMat.at&lt;float&gt;(i, 0);
				float y = detectionMat.at&lt;float&gt;(i, 1);
				float width = detectionMat.at&lt;float&gt;(i, 2);
				float height = detectionMat.at&lt;float&gt;(i, 3);
				int xLeftBottom = static_cast&lt;int&gt;((x - width / 2) * frame.cols);
				int yLeftBottom = static_cast&lt;int&gt;((y - height / 2) * frame.rows);
				int xRightTop = static_cast&lt;int&gt;((x + width / 2) * frame.cols);
				int yRightTop = static_cast&lt;int&gt;((y + height / 2) * frame.rows);
				Rect object(xLeftBottom, yLeftBottom,
					xRightTop - xLeftBottom,
					yRightTop - yLeftBottom);
				rectangle(frame, object, Scalar(0, 255, 0));
				if (objectClass &lt; classNamesVec.size())
				{
					ss.str(&quot;&quot;);
					ss &lt;&lt; confidence;
					String conf(ss.str());
					String label = String(classNamesVec[objectClass]) + &quot;: &quot; + conf;
					int baseLine = 0;
					Size labelSize = getTextSize(label, FONT_HERSHEY_SIMPLEX, 0.5, 1, &amp;baseLine);
					rectangle(frame, Rect(Point(xLeftBottom, yLeftBottom),
						Size(labelSize.width, labelSize.height + baseLine)),
						Scalar(255, 255, 255), -1);
					putText(frame, label, Point(xLeftBottom, yLeftBottom + labelSize.height),
						FONT_HERSHEY_SIMPLEX, 0.5, Scalar(0, 0, 0));
				}
			}
		}*/
		imshow(&quot;YOLOv3: Detections&quot;, frame);
		video_read &gt;&gt; frame;
		if (waitKey(1) &gt;= 0) break;
	}
}
</code></pre>
<pre><code class="language-c++">#opencv3.4.1 caffe googlenet 
#include&lt;opencv2\opencv.hpp&gt;
#include&lt;vector&gt;
#include&lt;string&gt;

using namespace cv;
using namespace dnn;
using namespace std;

String model_file = &quot;bvlc_googlenet.caffemodel&quot;;
String model_txtfile = &quot;deploy.prototxt&quot;;
String labels_file = &quot;synset_words.txt&quot;;

vector&lt;String&gt;readLabels();



int main(int arc, char** argv) {

	Mat src = imread(&quot;scr1.jpg&quot;);
	namedWindow(&quot;input&quot;, CV_WINDOW_AUTOSIZE);
	imshow(&quot;input&quot;, src);

	//¶ÁÈ¡Ä£ÐÍµÄÀà±ð£šÎÄ±Ÿ£©
	vector&lt;String&gt; labels = readLabels();


	//¶ÁÈ¡google_netµÄÄ£ÐÍºÍÃèÊöÎÄŒþ
	Net net = readNetFromCaffe(model_txtfile, model_file);
	if (net.empty()) {

		printf(&quot;read caffee model data failure\n&quot;);

		return -1;

	}

	//œ«ÍŒÏñ×ªÎªgoogle_netÍøÂçÊäÈëµÄ¶ÔÏó£¬ÓÉÃèÊöÎÄŒþ¿ÉÖª£¬ÍŒÏñ³ßŽçÍ³Ò»Îª224*224
	Mat inputBlob = blobFromImage(src, 1.0, Size(224, 224), Scalar(104, 117, 123));
	//œøÐÐÇ°ÏòŽ«²¥£¬ÓÉÃèÊöÎÄŒþ¿ÉÖª£¬µÚÒ»²ãÓÃÁË10žöŸí»ý²ã£¬ÌáÈ¡ÍŒÏñ10ÖÖ²»Í¬µÄÌØÕ÷
	Mat prob;

	for (int i = 0; i &lt; 10; i++) {
		net.setInput(inputBlob, &quot;data&quot;);
		prob = net.forward(&quot;prob&quot;);//×îºóÒ»²ãµÄÊä³öÎª¡°prob¡±
	}


	//Êä³ö
	//µÃµœµÄžÅÂÊÖµÎª1ÐÐ1000ÁÐµÄ
	Mat promat = prob.reshape(1, 1);
	Point classLoc;
	double classProb;
	minMaxLoc(promat, NULL, &amp;classProb, NULL, &amp;classLoc);
	printf(&quot;current image classification: %s,probablity %f\n&quot;, labels.at(classLoc.x).c_str(), classProb);

	putText(src, labels.at(classLoc.x), Point(20, 20), FONT_HERSHEY_COMPLEX, 1.0, Scalar(0, 0, 255), 2);

	imshow(&quot;output&quot;, src);

	waitKey(0);

	return 0;

}



//¶ÁÈ¡Ä£ÐÍµÄÀà±ð£šÎÄ±Ÿ£©

vector&lt;String&gt;readLabels() {
	vector&lt;String&gt;classNames;
	ifstream fp(labels_file);//Žò¿ªÎÄŒþ
	if (!fp.is_open()) {//ÎÄŒþÃ»Žò¿ª
		printf(&quot;could not open the file &quot;);
		exit(-1);
	}

	string name;

	while (!fp.eof()) {//ÎÄŒþÃ»¶ÁµœœáÎ²
		getline(fp, name);//µÃµœÃ¿Ò»ÐÐ£¬·ÅµœnameÖÐ
		if (name.length()) {//·Ç¿ÕÐÐ
			classNames.push_back(name.substr(name.find(' ') + 1));//
		}
	}

	fp.close();
	return classNames;

}
</code></pre>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[linux环境配置]]></title>
        <id>https://bixiaopeng0.github.io/post/linux-huan-jing-pei-zhi1</id>
        <link href="https://bixiaopeng0.github.io/post/linux-huan-jing-pei-zhi1">
        </link>
        <updated>2019-10-07T05:27:04.000Z</updated>
        <content type="html"><![CDATA[<ul>
<li>
<p>install googlepinyin</p>
<ul>
<li>卸载IBUS</li>
</ul>
<pre><code class="language-c">sudo apt-get  purge  ibus
</code></pre>
<ul>
<li>打开ubuntu软件中心，搜素fcitx并安装</li>
<li>sudo apt-get install fcitx-googlepinyin</li>
<li>将输入法系统换成fcitx</li>
<li>重启reboot</li>
<li>输入法简体和繁体相互切换  crtl+shift+f</li>
</ul>
</li>
<li>
<p>安装nvidia显卡驱动</p>
<ul>
<li>之前想通过下载驱动安装，但是安装的时候提示脚本失败，在网上有没找到相应的解决方法。</li>
<li>通过ubuntu驱动更新，自动安装</li>
<li>安装完了  reboot</li>
<li>在终端敲入 nvidia-smi 如果显示相应的内容则安装成功。</li>
</ul>
</li>
<li>
<p>typora</p>
<ul>
<li>直接进官网，会有相应的提示进行安装</li>
</ul>
</li>
<li>
<p>禁止ubuntu自动更新</p>
<ul>
<li>之前ubuntu自动更新，把显卡驱动整坏了，坑死我了</li>
<li>在软件更新里面，设置成never</li>
</ul>
</li>
<li>
<p>安装cuda</p>
<ul>
<li>
<p>sudo sh cuda_9.0.176_384.81_linux.run 如果提示不支持compiler  在命令后面加上-override</p>
</li>
<li>
<p>添加环境变量</p>
<ul>
<li>
<p>gedit ~/.bashrc</p>
</li>
<li>
<pre><code class="language-bash">export PATH=/usr/local/cuda-9.1/bin:$PATH
export LD_LIBRARY_PATH=/usr/local/cuda-9.1/lib64:$LD_LIBRARY_PATH
</code></pre>
</li>
<li>
<pre><code class="language-bash">source ~/.bashrc
</code></pre>
</li>
</ul>
</li>
</ul>
</li>
<li>
<p>安装cudnn</p>
<ul>
<li>
<p>tar -xzvf cudnn-*</p>
<pre><code>sudo cp cuda/include/cudnn.h /usr/local/cuda/include
sudo cp cuda/lib64/libcudnn* /usr/local/cuda/lib64
sudo chmod a+r /usr/local/cuda/include/cudnn.h /usr/local/cuda/lib64/libcudnn*
</code></pre>
</li>
</ul>
</li>
<li>
<p>anaconda</p>
<ul>
<li>
<pre><code>bash Anaconda3-4.2.0-Linux-x86_64.sh
</code></pre>
</li>
<li>
<p>在安装过程中添加环境变量</p>
</li>
<li>
<p>生效需要重启</p>
</li>
</ul>
</li>
<li>
<p>pycharm</p>
<ul>
<li>
<p>安装直接提取出来就行</p>
</li>
<li>
<p>创建快捷方式</p>
<ul>
<li>
<pre><code>cd /usr/share/applications
sudo gedit pycharm.desktop
</code></pre>
</li>
<li></li>
</ul>
</li>
</ul>
<p>[Desktop Entry]<br>
Version=1.0<br>
Type=Application<br>
Name=Pycharm<br>
Icon=/home/bi/Downloads/pycharm-community-2019.2.2/bin/pycharm.png<br>
Exec=sh /home/bi/Downloads/pycharm-community-2019.2.2/bin/pycharm.sh<br>
MimeType=application/x-py;<br>
Name[en_US]=pycharm</p>
<ul>
<li>
<p>gcc版本</p>
</li>
<li>
<pre><code>gcc -v   查看版本
</code></pre>
</li>
<li>
<pre><code class="language-c">gcc versions later than 6 are not supported!
gcc 降级
$ sudo apt-get install -y gcc-4.8
$ sudo apt-get install -y g++-4.8
$ cd /usr/bin
$ sudo rm gcc
$ sudo ln -s gcc-4.8 gcc
$ sudo rm g++
$ sudo ln -s g++-4.8 g++
</code></pre>
</li>
</ul>
</li>
<li>
<p>cmake cmake-gui</p>
<ul>
<li>sudo apt-get  install cmake</li>
<li>sudo apt-get install cmake-qt-gui</li>
</ul>
</li>
<li>
<p>caffe</p>
<ul>
<li>最近ubnutu18.04安装caffe-ssd-mobilnet遇到各种坑，记录一下这个过程遇到的问题。</li>
<li>cmake时候，提示gcc版本过高，解决：gcc 降级</li>
<li>cmake过程，缺少依赖库，解决：安装相应的库</li>
<li>cmake过程，部分包与anaconda的包冲突，解决：在安装caffe时将anaconda的环境变量去掉。</li>
<li>编译过程中，到了90左右，报错了，显示boost未定义，这个是最终换成ubuntu16.04解决的。</li>
<li>在ubuntu16.04编译过程中，异常顺利，没有遇到之前的问题，不过也遇到其他一些问题
<ul>
<li>运行demo.py  提示错误 1vs 4....，把某个文件里面的caffe的注释取消即可，具体文件记不太清楚了。</li>
<li>在转换voc数据集到指定格式时，运行create_data.sh，提示wu_inspire这个类的未定义，但是删了这个类之后，其他类也没与问题啊，之前在其他网络用这个数据集，也没有问题的 ？？？</li>
<li>训练时，提示内存溢出，把batch_size改小一点即可。</li>
</ul>
</li>
</ul>
</li>
<li></li>
</ul>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[Linux 指令]]></title>
        <id>https://bixiaopeng0.github.io/post/linux-zhi-ling1</id>
        <link href="https://bixiaopeng0.github.io/post/linux-zhi-ling1">
        </link>
        <updated>2019-09-24T07:06:30.000Z</updated>
        <content type="html"><![CDATA[<!--more-->
<ul>
<li>
<p>运行.run文件</p>
<ul>
<li>chmod +x xx.run提供权限</li>
<li>./  xx.run 运行   最好前面加上权限sudo</li>
</ul>
</li>
<li>
<p>解压文件 tar -xzvf XXX.tgz</p>
</li>
<li>
<p>复制文件 cp xx   xx</p>
</li>
<li>
<p>创建目录 eg:   mkdir /usr/local/....</p>
</li>
<li>
<p>/usr:系统级目录 /usr/local 用户程序级目录</p>
</li>
<li>
<p>cd</p>
<ul>
<li>cd cuda 进入cuda目录</li>
</ul>
</li>
<li>
<p>crtl+空格  换输入法</p>
</li>
<li>
<p>查看nvidia板卡信息  watch -n 1 nvidia-smi<br>
待续。。。2019/09/24</p>
</li>
<li>
<p>reboot重启</p>
</li>
</ul>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[常犯的错误]]></title>
        <id>https://bixiaopeng0.github.io/post/chang-fan-de-cuo-wu</id>
        <link href="https://bixiaopeng0.github.io/post/chang-fan-de-cuo-wu">
        </link>
        <updated>2019-09-13T00:52:58.000Z</updated>
        <content type="html"><![CDATA[<h3 id="vs">vs</h3>
<ul>
<li>今天在调无人机代码，提示类UAV不存在，但是在头文件中是有引用的，然后调了好长时间才发现两个类的头文件宏定义判断是一样的。</li>
<li>代码比较多的时候。不要用using namespace,之前因为这个问题，opencv和windows的函数发生了冲突，然后一个一个的加上去的。</li>
</ul>
<h3 id="qt">qt</h3>
<ul>
<li>qt在中文支持不好，尽量不要用中文写。</li>
<li>命名不要用拼音，编号，否则二次开发比较麻烦。</li>
</ul>
<h3 id="opencv">opencv</h3>
<ul>
<li>多人开发，opencv版本要一致，这个之前也被坑过</li>
</ul>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[ Socket传输图片]]></title>
        <id>https://bixiaopeng0.github.io/post/socket-chuan-shu-tu-pian1</id>
        <link href="https://bixiaopeng0.github.io/post/socket-chuan-shu-tu-pian1">
        </link>
        <updated>2019-09-07T09:28:54.000Z</updated>
        <content type="html"><![CDATA[<!--more-->
<p>最近做一个项目，需要c++端和python端进行通信，c++将图片传到python端，然后python把处理完数据传回到c++端，然后就尝试了用socket通信，c++作为客户端，python作为服务器。</p>
<h2 id="文件流发送">文件流发送</h2>
<p>刚开始是用本地图片测试的，c++读取图片，发送到python端，然后python将图片数据写进硬盘，然后进行读取，然后测试</p>
<pre><code class="language-c++">#include&lt;iostream&gt;
#include&lt;winsock.h&gt;
#pragma comment(lib,&quot;ws2_32.lib&quot;)
using namespace std;
void initialization();
int main() {


	int cnt = 0;
	//定义长度变量
	int send_len = 0;
	int recv_len = 0;
	//定义发送缓冲区和接受缓冲区
	char send_buf[2000];
	char recv_buf[2000];
	//定义服务端套接字，接受请求套接字
	SOCKET s_server;
	//服务端地址客户端地址
	SOCKADDR_IN server_addr;
	initialization();
	//填充服务端信息
	server_addr.sin_family = AF_INET;
	server_addr.sin_addr.S_un.S_addr = inet_addr(&quot;127.0.0.1&quot;);
	server_addr.sin_port = htons(1234);
	//创建套接字
	s_server = socket(AF_INET, SOCK_STREAM, 0);
	if (connect(s_server, (SOCKADDR *)&amp;server_addr, sizeof(SOCKADDR)) == SOCKET_ERROR) {
		cout &lt;&lt; &quot;服务器连接失败！&quot; &lt;&lt; endl;
		WSACleanup();
	}
	else {
		cout &lt;&lt; &quot;服务器连接成功！&quot; &lt;&lt; endl;
	}
	//发送图片
	FILE *fq = fopen(&quot;2.jpg&quot;, &quot;rb&quot;);
	while (!feof(fq))
	{
		cout &lt;&lt; cnt++ &lt;&lt; endl;
		send_len = fread(send_buf, 1, sizeof(send_buf), fq);
		send(s_server, send_buf, 2000, 0);
	}
	send(s_server, &quot;exit&quot;, 2000, 0);





	//发送,接收数据
	while (1) {
		cout &lt;&lt; &quot;请输入发送信息:&quot;;
		cin &gt;&gt; send_buf;
		send_len = send(s_server, send_buf, 100, 0);
		if (send_len &lt; 0) {
			cout &lt;&lt; &quot;发送失败！&quot; &lt;&lt; endl;
			break;
		}
		recv_len = recv(s_server, recv_buf, 100, 0);
		if (recv_len &lt; 0) {
			cout &lt;&lt; &quot;接受失败！&quot; &lt;&lt; endl;
			break;
		}
		else {
			cout &lt;&lt; &quot;服务端信息:&quot; &lt;&lt; recv_buf &lt;&lt; endl;
		}

	}
	//关闭套接字
	closesocket(s_server);
	//释放DLL资源
	WSACleanup();
	return 0;
}
void initialization() {
	//初始化套接字库
	WORD w_req = MAKEWORD(2, 2);//版本号
	WSADATA wsadata;
	int err;
	err = WSAStartup(w_req, &amp;wsadata);
	if (err != 0) {
		cout &lt;&lt; &quot;初始化套接字库失败！&quot; &lt;&lt; endl;
	}
	else {
		cout &lt;&lt; &quot;初始化套接字库成功！&quot; &lt;&lt; endl;
	}
	//检测版本号
	if (LOBYTE(wsadata.wVersion) != 2 || HIBYTE(wsadata.wHighVersion) != 2) {
		cout &lt;&lt; &quot;套接字库版本号不符！&quot; &lt;&lt; endl;
		WSACleanup();
	}
	else {
		cout &lt;&lt; &quot;套接字库版本正确！&quot; &lt;&lt; endl;
	}
	//填充服务端地址信息

}
</code></pre>
<pre><code class="language-python">def deal_image(sock):
    # print(&quot;Accept connection from {0}&quot;.format(addr))  # 查看发送端的ip和端口
    cnt = 0
    new_filename = &quot;cat1.jpg&quot;
    fp = open(&quot;scr.jpg&quot;, 'wb')
    pxiel = 0
    #接受焦距信息
    data = sock.recv(200)
    string = data.decode('utf-8','ignore').split('\x00')[0]
    if string[0] == 'p':
        pxiel = int(string[1:3])
        print(pxiel)
    #接受图片数据
    data = sock.recv(2000)
    start_time = time.time()
    while True:
            # sock.send(cnt)
        # print(len(data))
        # print(data[0:4])
        string = data.decode('utf-8','ignore').split('\x00')[0]
        # print(type(data),type(string))
        if string=='end':
            print(&quot;over&quot;)
            break

        fp.write(data)  # 写入图片数据
        data = sock.recv(2000)
            # s= dete_img.test_img(img_path).cpu().numpy()[0]
            # sock.send(str(key_value[s]).encode('utf-8'))      #.decode('utf-8')).encode('utf-8')
            #print(s)
    fp.close()
    end_time = time.time()
    #发送识别到的信息
    str_list = [&quot;1&quot;,&quot;2&quot;]
    str = &quot;1,jingling,0.34,100,200,300,400,23,t16,0.34,100,200,300,400,23&quot;
    sock.send(str.encode())
    str = &quot;0.34,100,200,300,400,23&quot;
    sock.send(str.encode())
    print(end_time-start_time)
    sock.close()


</code></pre>
<h2 id="编码发送">编码发送</h2>
<p>后来意识到把图片数据写到硬盘和之前的文件夹通信一样，不是很安全，所以就想对视频中图片进行编码，然后服务器解码即可。</p>
<pre><code class="language-c++">vector&lt;string&gt; sendsocket::getSocketInfo(cv::Mat img, int p)
{
	int x1 = (img.cols - img.rows) / 2;
	cv::Rect rect = cv::Rect(x1, 0, img.rows, img.rows);
	img = img(rect);
	std::vector&lt;uchar&gt; data_encode;
	std::vector&lt;int&gt; quality;
	quality.push_back(CV_IMWRITE_JPEG_QUALITY);
	quality.push_back(90);//进行90%的压缩
	imencode(&quot;.jpg&quot;, img, data_encode, quality);//将图像编码
	int nSize = data_encode.size();
	string s1 = &quot;p,&quot; + to_string(p) + &quot;,&quot; + to_string(nSize) + &quot;,&quot;;
	const char* c = s1.c_str();
	for (long int i = 0; i &lt; nSize; i++)
	{
		encodeImg[i] = data_encode[i];
	}
	send(sock, c, 200, 0);
	//发送图片
	long int send_num = 0;
	while (send_num &lt; nSize)
	{
		send(sock, encodeImg + send_num, 2000, 0);
		send_num += 2000;
	}

	send(sock, &quot;end&quot;, 100, 0);
	//接受数据
	recv(sock, recv_buf, 400, 0);
	vector&lt;string&gt; img_info = split(recv_buf, &quot;,&quot;);
	memset(recv_buf, '\0', sizeof(recv_buf));
	memset(&amp;encodeImg, '\0', sizeof(encodeImg));  //初始化结构体
	return img_info;

}
</code></pre>
<h2 id="多线程编码发送">多线程编码发送</h2>
<p>之前通信顺序是串行的，感觉这样速度很慢，所以想用多线程提高一下速度，如果图片数据比较小还好，速度只被网络处理时间限制，如果图片很大的话，socket传输时间还是挺长的。</p>
<p>在python中，将网络和socket通信各自给了一个线程，理想情况下是，网络处理完图片，就可以处理下一张图片，不需要等待。</p>
<pre><code class="language-python">def dealSocketMessage():
    global image,pxiel,sock,recv_flag,send_flag,lock

    # while True:
    #     cv2.waitKey(100)
    #     print(&quot;test socket&quot;)
    cnt = 0
    while True:
        # print(&quot;recv_flag&quot;,recv_flag)
        if recv_flag:
            start_time_s = time.time()
            start_time = time.time()
            data = sock.recv(200)
            print(&quot;time1&quot;,time.time()-start_time)
            string = data.decode('utf-8', 'ignore').split(',')
            if string[0] == 'p':
                pxiel = int(string[1])
            img_bytes = bytes()
            start_time = time.time()
            while True:
                start_time1= time.time()
                data = sock.recv(2000)
                print(&quot;recve img time&quot;,time.time()-start_time1)
                string = data.decode('utf-8', 'ignore').split('\x00')[0]
                if string == &quot;end&quot;:
                    # print(&quot;接受成功&quot;)
                    lock.acquire()
                    img = np.asarray(bytearray(img_bytes), dtype=&quot;uint8&quot;)
                    img = cv2.imdecode(img, cv2.IMREAD_COLOR)

                    image = copy.deepcopy(img)
                    lock.release()
                    break
                img_bytes = img_bytes + data
            print(&quot;time2&quot;, time.time() - start_time)
            str = &quot;,&quot;.join(str_list) + &quot;,&quot;
            # 发送识别到的数据
            sock.send(str.encode())
            lock.acquire()
            recv_flag = False
            lock.release()
            cnt+=1
            if cnt==101:
                cv2.waitKey()
            print(&quot;recv time &quot;,(time.time()-start_time_s))
        else:
            cv2.waitKey(1)
</code></pre>
<h3 id="问题">问题</h3>
<p>1、设置一个标志位，True代表socket接受图片数据，当为false表示接受完成。我是一直判断的，但是发现这样处理速度在1fps，当时很郁闷，后来改成每隔nms检测一次标志位就好了，但是具体原因还没有找到。</p>
<p>2、如果图片数据比较小，可以达到顶速，也就是网络的速度，但是图片为1080p时，如果网络已经处理完图片，但是socket还在接受数据，这时会出现几百毫秒的卡顿，这个问题也没搞明白。</p>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[Python调用动态链接库]]></title>
        <id>https://bixiaopeng0.github.io/post/python-diao-yong-dong-tai-lian-jie-ku</id>
        <link href="https://bixiaopeng0.github.io/post/python-diao-yong-dong-tai-lian-jie-ku">
        </link>
        <updated>2019-08-23T14:34:47.000Z</updated>
        <content type="html"><![CDATA[<!-- more --> 
<h1 id="dll生成">DLL生成</h1>
<ol>
<li>选择win32控制台项目</li>
<li>选择dll项目</li>
<li>除了空项目外，导出符号，预编译头文件，安全周期检查都要选择。</li>
<li>如果预编译头文件出问题，在属性-c++-预编译头选型里不使用预编译头文件</li>
</ol>
<p>按照以上步骤走就可以生成成功了，vs也会生成模板，可以按照提示做</p>
<h1 id="调用dll">调用DLL</h1>
<p>首先抛出今天遇到的问题</p>
<ol>
<li>
<pre><code class="language-c++">AttributeError: function 'recog_color' not found
</code></pre>
</li>
<li>
<pre><code class="language-c++">ctypes.ArgumentError: argument 7: &lt;class 'TypeError'&gt;: Don't know how to convert parameter 7
</code></pre>
</li>
</ol>
<p>第一个问题是建立的c++工程，在导出头文件前面加上</p>
<pre><code class="language-c++">#ifdef COLOR_RECOG_DLL_EXPORTS
#define COLOR_RECOG_DLL_API  extern &quot;C&quot; __declspec(dllexport)
#else
#define COLOR_RECOG_DLL_API  extern &quot;C&quot; __declspec(dllimport)
#endif
</code></pre>
<p>这一段除了extern &quot;C&quot;是系统自己生成的，在前面加上extern &quot;c&quot;就行</p>
<p>第二个问题是参数问题</p>
<ul>
<li>第一个出问题是因为参数不匹配，传入的参数是float型，实际参数是int型</li>
<li>Mat类型不可以直接传，需要变化一下，具体代码在github里面</li>
</ul>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[Git指令]]></title>
        <id>https://bixiaopeng0.github.io/post/git-zhi-ling1</id>
        <link href="https://bixiaopeng0.github.io/post/git-zhi-ling1">
        </link>
        <updated>2019-07-21T14:12:34.000Z</updated>
        <content type="html"><![CDATA[<!-- more -->
<h1 id="git-指令">git 指令</h1>
<ul>
<li>
<p>配置用户名即邮箱，以及验证</p>
<!--more-->
<pre><code class="language-python">git config --global user.name &quot;  &quot; #配置用户名
#global对电脑上所有的仓库起作用，不加的话只对当前仓库有效
git config --global user.email &quot;  &quot; #配置邮箱
git config user.name
git config user.email #查看配置后的用户名和邮箱
</code></pre>
</li>
<li>
<p>将GitHub上的仓库克隆到本地</p>
<pre><code class="language-python">cd GitHub_test/   #进入GitHub目录
git clone https://github.com/bixiaopeng0/bixiaopeng0.github.io.git
</code></pre>
</li>
<li>
<p>将本地文件git到GitHub远程仓库</p>
<pre><code class="language-python">git add something  #这里可以是具体某个文件或者文件夹  
				   #将工作区的内容添加到暂存区
git commit -m &quot;备注&quot;#将暂存区的内容提交到仓库区
git push origin master #将仓库去的内容推送到GitHub上

</code></pre>
</li>
<li>
<p>删除文件</p>
<pre><code class="language-python">git rm -r --cached target  删除target文件夹
git commit -m '删除了target' 
git push -u origin master 将本次更改更新到github项目上去
</code></pre>
</li>
<li>
<p>创建一个新的仓库</p>
<pre><code class="language-python">git init
git add README.md
git commit -m &quot;first commit&quot;
git remote add origin https://github.com/bixiaopeng0/Statistical-learning-method.git
git push -u origin master
</code></pre>
</li>
</ul>
<p>time:2019/11/03</p>
<hr>
<p>后续。。。。</p>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[CMake]]></title>
        <id>https://bixiaopeng0.github.io/post/cmake</id>
        <link href="https://bixiaopeng0.github.io/post/cmake">
        </link>
        <updated>2019-07-20T15:24:00.000Z</updated>
        <summary type="html"><![CDATA[<h1 id="cmake">cmake</h1>
<h2 id="简介">简介</h2>
<p>Cmake是众多make工具中的一种，对代码进行编译，测试以及打包操作的跨平台工具，cmake在工作时，主要分为两部分。首先，编写cmaklists.txt文件，然后用cmake命令将CMakeLists.txt文件转化为make所需要的makefile文件，最后用make命令编译源码生成可执行程序或共享库</p>
]]></summary>
        <content type="html"><![CDATA[<h1 id="cmake">cmake</h1>
<h2 id="简介">简介</h2>
<p>Cmake是众多make工具中的一种，对代码进行编译，测试以及打包操作的跨平台工具，cmake在工作时，主要分为两部分。首先，编写cmaklists.txt文件，然后用cmake命令将CMakeLists.txt文件转化为make所需要的makefile文件，最后用make命令编译源码生成可执行程序或共享库</p>
<!--more-->
<h2 id="优点">优点</h2>
<ul>
<li>源码是跨平台的，可以直接用cmake在linux下编译</li>
<li>源码和工程文件分开，便于代码版本管理</li>
</ul>
<h2 id="常用命令">常用命令</h2>
<ul>
<li>
<p>指定cmake最小版本,防止因为cmake语法不向下兼容。</p>
<pre><code class="language-cmake">cmake_minimum_required(VERSION 3.4.1)
</code></pre>
</li>
<li>
<p>设置项目名称</p>
<pre><code class="language-cmake">project(demo)
</code></pre>
</li>
<li>
<p>设置编译类型</p>
<pre><code class="language-cmake">add_executable(demo demo.cpp) # 生成可执行文件
add_library(common STATIC util.cpp) # 生成静态库
add_library(common SHARED util.cpp) # 生成动态库或共享库
</code></pre>
</li>
<li>
<p>搜寻文件</p>
<pre><code class="language-cmake">file(GLOB APP_LIB &quot;*.h&quot; &quot;*.cpp&quot;)
add_library(uavDetect SHARED ${APP_LIB})
</code></pre>
</li>
<li>
<p>添加链接库</p>
<pre><code class="language-cmake">target_link_libraries(uavDetect ${OpenCV_LIBS})
</code></pre>
</li>
<li>
<p>查找依赖包</p>
<pre><code class="language-cmake">find_package(OpenCV)
</code></pre>
</li>
</ul>
<h2 id="注意事项">注意事项</h2>
<ul>
<li>之前工程文件调用了opencv库，在编译的时候是默认用了X86架构，但是opencv只有64位的，所以一直报错，最初以为cmake没有找到opencv的路径，最后发现是默认调用了win32编译器，在gui界面换一下就好，然后在gui里生成。</li>
<li>在使用时，最好把源码和工程分开，这样相对好管理。</li>
<li>在gui里配置，首先选择编译器vs还是mingw等等，其实要选对架构，x64 or x86</li>
</ul>
<p>emmmmm,好多语法还不熟悉，以后慢慢补充吧</p>
<p>后续。。。。。</p>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[暗通道去雾]]></title>
        <id>https://bixiaopeng0.github.io/post/an-tong-dao-qu-wu</id>
        <link href="https://bixiaopeng0.github.io/post/an-tong-dao-qu-wu">
        </link>
        <updated>2019-02-17T08:17:25.000Z</updated>
        <summary type="html"><![CDATA[<h1 id="暗通道去雾">暗通道去雾</h1>
<h2 id="序言">序言</h2>
<p>毕业设计选了用fpga做图像处理，之前在做车牌识别的时候，碰巧发现做去雾处理的，发现这个蛮有意思的，当时考研没有时间呢，现在正好借毕设把这个想法实现一下。其中图像去雾处理，网上有好多方法，比较了一下，何恺明的暗通道去雾应该是效果最好的，后来的人也对他的算法进行了改进，基本能达到比较不错的结果。</p>
]]></summary>
        <content type="html"><![CDATA[<h1 id="暗通道去雾">暗通道去雾</h1>
<h2 id="序言">序言</h2>
<p>毕业设计选了用fpga做图像处理，之前在做车牌识别的时候，碰巧发现做去雾处理的，发现这个蛮有意思的，当时考研没有时间呢，现在正好借毕设把这个想法实现一下。其中图像去雾处理，网上有好多方法，比较了一下，何恺明的暗通道去雾应该是效果最好的，后来的人也对他的算法进行了改进，基本能达到比较不错的结果。</p>
<!--more-->
<h2 id="原理">原理</h2>
<h3 id="暗通道先验理论">暗通道先验理论</h3>
<p>暗通道图像就是三个颜色分量的最小值组成一副灰度图像，</p>
<figure data-type="image" tabindex="1"><img src="https://picture-1257139399.cos.ap-chengdu.myqcloud.com/picture/2019-02-17_160713.png?q-sign-algorithm=sha1&amp;q-ak=AKID7yDqvpy6fT84L5RY99s3Aa5V3oCmfe4m&amp;q-sign-time=1550408958;1550410758&amp;q-key-time=1550408958;1550410758&amp;q-header-list=&amp;q-url-param-list=&amp;q-signature=ea8b51c96f156fd856ccea2da4d2c73cb860bc28&amp;x-cos-security-token=46bb6494b4d4a2b489dac965cc969b7c72724dd910001" alt=""></figure>
<p>对于没有雾的图像J_dark趋于零，这一结果是何博士分析了5000多张正常图片的暗通道结果，而对于有雾图像并不满足之一验证。</p>
<p>#####公式</p>
<p>在去雾处理中，下面去雾模型被广泛应用</p>
<figure data-type="image" tabindex="2"><img src="https://picture-1257139399.cos.ap-chengdu.myqcloud.com/2019-02-17_162722.png?q-sign-algorithm=sha1&amp;q-ak=AKID8LhGrumQQ1ZEbIh7Dqq0cofTQeUMHa7E&amp;q-sign-time=1550409058;1550410858&amp;q-key-time=1550409058;1550410858&amp;q-header-list=&amp;q-url-param-list=&amp;q-signature=dd8d45457743455af59d11f1368d425e8589c4bb&amp;x-cos-security-token=0b0aa22673862f735f7c3d4904442bcf2e6c623b10001" alt=""></figure>
<p>其中I(x)代表输入的雾图，J(x)代表去雾后的图像，t(x)为透射图，A为大气参数。现在只有I(x)知道，想要求出来J（x），还要把tx,A求出来才可以</p>
<p>对公式两边求最小值操作，因为正常图像的暗通道趋于零，所以可以化简为</p>
<figure data-type="image" tabindex="3"><img src="https://picture-1257139399.cos.ap-chengdu.myqcloud.com/2019-02-17_163812.png?q-sign-algorithm=sha1&amp;q-ak=AKIDGj7TQTUgoVMBXcL60BJ5mBwlH5HUcwAr&amp;q-sign-time=1550409080;1550410880&amp;q-key-time=1550409080;1550410880&amp;q-header-list=&amp;q-url-param-list=&amp;q-signature=379fc9f2731e577ef0ccf7dfb00eb2bbb4a52bda&amp;x-cos-security-token=4a81740da7bff9a4b68e0be5482e8724c9992f0210001" alt=""></figure>
<p>因为在实际景物中存在一定雾气，因此在前面加入一个参数w，这个因子一般取0.95</p>
<figure data-type="image" tabindex="4"><img src="https://picture-1257139399.cos.ap-chengdu.myqcloud.com/2019-02-17_163823.png?q-sign-algorithm=sha1&amp;q-ak=AKIDUOrbFTLazXSEj9UtABZAhpFe0WE0pfci&amp;q-sign-time=1550409124;1550410924&amp;q-key-time=1550409124;1550410924&amp;q-header-list=&amp;q-url-param-list=&amp;q-signature=8fc42281be3dfdced901456e2c17e1329ba081d1&amp;x-cos-security-token=17fbbb9349de21deb17579a3843953c6177d1eec10001" alt=""></figure>
<p>而大气参数A,可以根据暗通道求出，最后的去雾公式为</p>
<figure data-type="image" tabindex="5"><img src="https://picture-1257139399.cos.ap-chengdu.myqcloud.com/2019-02-17_170039.png?q-sign-algorithm=sha1&amp;q-ak=AKIDwXWxuK2kNo7MoNHpdeWx6zyIhkDnwSS8&amp;q-sign-time=1550409148;1550410948&amp;q-key-time=1550409148;1550410948&amp;q-header-list=&amp;q-url-param-list=&amp;q-signature=4a547972513a26e23bdf7474f16408213f2a446e&amp;x-cos-security-token=9929c82600f243c1175a725c6657be552fa6bdd410001" alt=""></figure>
<p>为了防止J(x)过大超过阈值，tx不能太小，因此对tx设置一个最小值t0=0.1</p>
<h2 id="opencv实现">opencv实现</h2>
<h3 id="求暗通道">求暗通道</h3>
<p>求三个通道中的最小值即可，用at遍历，然后输出为单通道图片,最后进行最小值滤波，最小值滤波在</p>
<figure data-type="image" tabindex="6"><img src="https://picture-1257139399.cos.ap-chengdu.myqcloud.com/picture/%E6%9A%97%E9%80%9A%E9%81%93.png?q-sign-algorithm=sha1&amp;q-ak=AKID0MkcaDc229p2Fh2YUhnhOI4Vkj1b7OX6&amp;q-sign-time=1550408910;1550410710&amp;q-key-time=1550408910;1550410710&amp;q-header-list=&amp;q-url-param-list=&amp;q-signature=a8fbecaa564d13375dfb7117a2e4c09f2fb3f4ff&amp;x-cos-security-token=e283a10d42652c7609d002393de1e8b653d540f010001" alt=""></figure>
<pre><code class="language-c++">int FindDarkChannel(Mat* scr_img, Mat* dst_img)
{
	uchar r, g, b;
	for (int row = 0; row &lt; scr_img-&gt;rows; row++)
	{
		for (int col = 0; col &lt; scr_img-&gt;cols; col++)
		{
			r = scr_img-&gt;at&lt;Vec3b&gt;(row, col)[0];
			g = scr_img-&gt;at&lt;Vec3b&gt;(row, col)[1];
			b = scr_img-&gt;at&lt;Vec3b&gt;(row, col)[2];
			if (r &lt; g&amp;&amp;r &lt; b)
			{
				dst_img-&gt;at&lt;uchar&gt;(row,col) = r;
			}
			else if (g &lt; r&amp;&amp;g &lt; b)
			{
				dst_img-&gt;at&lt;uchar&gt;(row, col) = g;
			}
			else
			{
				dst_img-&gt;at&lt;uchar&gt;(row, col) = b;
			}
		}
	}
	return 0;
}

</code></pre>
<h3 id="获取大气光值a">获取大气光值A</h3>
<p>1、首先把暗通道分成一千份，然后找到这千分之一最亮的像素区域。</p>
<p>2、从这块区域中找到最亮的点就是大气光强。</p>
<p>分成1000份是为了防止把一些非天空区域的白色物体当成大气光值</p>
<pre><code class="language-c++">int GetMosphereLight(Mat* dark_img, Mat* scr_img, int* A)
{
	int top_size = dark_img-&gt;cols*dark_img-&gt;rows / 1000;
	//cout &lt;&lt; dark_img-&gt;cols*dark_img-&gt;rows &lt;&lt; endl;
	//cout &lt;&lt; &quot;top_size::&quot; &lt;&lt; top_size &lt;&lt; endl;
	int sum_pxiel[1010] = {0};  //多余的储存剩下的像素
	int max_num = 0,cnt_num=0,cnt_pixel=0;//0开始

	for (int row = 0; row &lt; dark_img-&gt;rows; row++)
	{
		for (int col = 0; col &lt; dark_img-&gt;cols; col++)
		{
			sum_pxiel[cnt_num] += dark_img-&gt;at&lt;uchar&gt;(row, col);
			cnt_pixel++;
			if (cnt_num == 0)
				cout &lt;&lt; cnt_pixel &lt;&lt; endl;

			if (cnt_pixel == top_size)
			{
				//cout &lt;&lt; &quot;cnt_num&quot; &lt;&lt; cnt_num &lt;&lt;&quot;  &quot;&lt;&lt;sum_pxiel[cnt_num]&lt;&lt; endl;
				cnt_pixel = 0;
				cnt_num++;

			}
		}
	}
	max_num = 0;

	//debug
	for (int i = 0; i &lt; 100; i++)
	{
		cout &lt;&lt; i &lt;&lt; &quot;   &quot; &lt;&lt; sum_pxiel[i] &lt;&lt; endl;
	}

	for (int i = 0; i &lt; 1000; i++)
	{
	
		if (sum_pxiel[i] &gt; sum_pxiel[max_num])
		{
			max_num = i;
		}
	}
	cout &lt;&lt; &quot;max_num         &quot; &lt;&lt; max_num &lt;&lt; endl;

	int row = max_num * top_size / dark_img-&gt;cols;
	int col = max_num * top_size % dark_img-&gt;cols;

	cout &lt;&lt; &quot;row  &quot; &lt;&lt; row &lt;&lt; &quot;col  &quot; &lt;&lt; col &lt;&lt; endl;

	int max_row = row, max_col = col;

	for (int i = 0; i &lt; top_size; i++)
	{
		//cout &lt;&lt; &quot;i==&quot; &lt;&lt; i &lt;&lt; &quot;  &quot; &lt;&lt; endl;
		int row_temp = row + i / dark_img-&gt;cols;
		int col_temp = col + i % dark_img-&gt;cols;
		if (dark_img-&gt;at&lt;uchar&gt;(row_temp,col_temp) &gt; dark_img-&gt;at&lt;uchar&gt;(max_row, max_col))
		{
			max_row = row_temp;
			max_col = col_temp;
		}

	}

	Mat a_img = scr_img-&gt;clone();
	Point a_point;
	a_point.x = max_col;
	a_point.y = max_row;
	circle(a_img, a_point, 10, (0, 255, 0),5);
	imshow(&quot;mark大气光值&quot;, a_img);

	for (int i = 0; i &lt; 3; i++)
	{
		A[i] = scr_img-&gt;at&lt;Vec3b&gt;(max_row, max_col)[i];
	}

	return 0;
}

</code></pre>
<figure data-type="image" tabindex="7"><img src="https://picture-1257139399.cos.ap-chengdu.myqcloud.com/picture/%E5%A4%A7%E6%B0%94%E5%85%89%E5%80%BC.png?q-sign-algorithm=sha1&amp;q-ak=AKIDdT58EP6Adb6UoDkuq3MrLe2lG6a7S3XP&amp;q-sign-time=1550408811;1550410611&amp;q-key-time=1550408811;1550410611&amp;q-header-list=&amp;q-url-param-list=&amp;q-signature=ddc2ce0a9fbd6168c56265bb553e70be755b6337&amp;x-cos-security-token=10cb6250dcba2d0373167fcf2946ec8c476023a210001" alt=""></figure>
<h3 id="获取透射图">获取透射图</h3>
<p>这个大气光值A是取得三个通道的平均值，这样可以加快运算速度，另外需要注意图像的类型</p>
<pre><code class="language-c++">int GetTransmission(Mat* scr_img, Mat* trans_img, int* A, float w)
{
	Mat trans_temp = Mat(scr_img-&gt;rows, scr_img-&gt;cols, CV_32FC3);
	float a_average = (float)(A[0] + A[1] + A[2]) / 3;
	if (a_average &gt; 220)
		a_average = 220;
	cout &lt;&lt; &quot;average&quot; &lt;&lt; a_average &lt;&lt; endl;
	for (int row = 0; row &lt; scr_img-&gt;rows; row++)
	{
		for (int col = 0; col &lt; scr_img-&gt;cols; col++)
		{
			for (int i = 0; i &lt; 3; i++)
			{
				trans_temp.at&lt;Vec3f&gt;(row, col)[i] = (float)scr_img-&gt;at&lt;Vec3b&gt;(row, col)[i] / a_average;
			}
		}
	}
	imshow(&quot;t_temp&quot;, trans_temp);

	Mat dark_t_img = Mat(scr_img-&gt;size(), CV_32FC1);
	FindDarkChannel_f(&amp;trans_temp, &amp;dark_t_img);
	imshow(&quot;t暗通道&quot;, dark_t_img);
	*trans_img = dark_t_img.clone();
	MinFilter_f(&amp;dark_t_img, trans_img, 6);
	imshow(&quot;t_min&quot;, *trans_img);

	int a = 0;
	for (int row = 0; row &lt; scr_img-&gt;rows; row++)
	{
		for (int col = 0; col &lt; scr_img-&gt;cols; col++)
		{
			if ((1 - w * trans_img-&gt;at&lt;float&gt;(row, col)) &lt; 0)
			{
				cout &lt;&lt; &quot;  ww&quot; &lt;&lt; endl;
				trans_img-&gt;at&lt;float&gt;(row, col) = 0;
			}
			else
				trans_img-&gt;at&lt;float&gt;(row, col) = 1 - w * trans_img-&gt;at&lt;float&gt;(row, col);
			if (trans_img-&gt;at&lt;float&gt;(row, col) &lt; 0.1)
			{
				//cout &lt;&lt; a++ &lt;&lt; endl;
				trans_img-&gt;at&lt;float&gt;(row, col) = 0.1;
			}
		}

	}


	return 0;
}

</code></pre>
<figure data-type="image" tabindex="8"><img src="https://picture-1257139399.cos.ap-chengdu.myqcloud.com/picture/%E6%9C%AA%E5%AF%BC%E5%90%91%E6%BB%A4%E6%B3%A2%E9%80%8F%E5%B0%84%E5%9B%BE.png?q-sign-algorithm=sha1&amp;q-ak=AKIDeBLJdgObcWvlGq4wqazqioBEvcEKuytt&amp;q-sign-time=1550408785;1550410585&amp;q-key-time=1550408785;1550410585&amp;q-header-list=&amp;q-url-param-list=&amp;q-signature=f11b2d5a1a3fb2e68663e8df266ee5af3f77be9a&amp;x-cos-security-token=fc5c45145d48463dd50e5dd3c051f92d671fd2ec10001" alt=""></figure>
<h3 id="导向滤波">导向滤波</h3>
<p>直接出来的透射图并不精细，用导向滤波后输出的透射图更加精细，导向滤波的引导图可以是灰度图，也可以是rgb图像。opencv里面有导向滤波的函数，但是需要经过编译后才可以使用，导向滤波的实现并不麻烦，这里可以自己实现</p>
<pre><code class="language-c++">int GuideFilter(Mat* scr_img, Mat* guide_img, Mat* final_img, int radius, float e)
{
	Mat guide_conver = Mat(guide_img-&gt;size(), CV_32FC1);
	guide_img-&gt;convertTo(guide_conver, CV_32FC1, 1.0 / 255);
	
	int size = radius * 2 + 1;
	//1
	Mat mean_i = Mat(guide_img-&gt;size(), CV_32FC1);
	boxFilter(guide_conver, mean_i, CV_32FC1, Size(size, size));
	Mat mean_p = Mat(guide_img-&gt;size(), CV_32FC1);
	boxFilter(*scr_img, mean_p, CV_32FC1, Size(size, size));
	Mat mean_ip = Mat(guide_img-&gt;size(), CV_32FC1);
	boxFilter(guide_conver.mul(*scr_img), mean_ip, CV_32FC1, Size(size, size));
	Mat mean_ii = Mat(guide_img-&gt;size(), CV_32FC1);
	boxFilter(guide_conver.mul(guide_conver), mean_ii, CV_32FC1, Size(size, size));;

	//2
	Mat var_i = Mat(guide_img-&gt;size(), CV_32FC1);
	var_i = mean_ii - mean_i.mul(mean_i);
	Mat cov_ip = Mat(guide_img-&gt;size(), CV_32FC1);
	cov_ip = mean_ip - mean_i.mul(mean_p);
	//3
	Mat a = Mat(guide_img-&gt;size(), CV_32FC1);
	a = cov_ip / (var_i + e);
	Mat b = Mat(guide_img-&gt;size(), CV_32FC1);
	b = mean_p - a.mul(mean_i);
	//4
	Mat mean_a = Mat(guide_img-&gt;size(), CV_32FC1);
	Mat mean_b = Mat(guide_img-&gt;size(), CV_32FC1);
	boxFilter(a, mean_a, CV_32FC1, Size(size, size));
	boxFilter(b, mean_b, CV_32FC1, Size(size, size));
	//5
	*final_img = mean_a.mul(guide_conver) + mean_b;
	return 0;
}
</code></pre>
<figure data-type="image" tabindex="9"><img src="https://picture-1257139399.cos.ap-chengdu.myqcloud.com/picture/%E5%AF%BC%E5%90%91%E6%BB%A4%E6%B3%A2%E7%AE%97%E6%B3%95.png?q-sign-algorithm=sha1&amp;q-ak=AKIDZ31keL0udBHfW2Zw2wPM0jrCM1F5qKrv&amp;q-sign-time=1550408726;1550410526&amp;q-key-time=1550408726;1550410526&amp;q-header-list=&amp;q-url-param-list=&amp;q-signature=3bcf936b516ef8aebb63a7c99583e70a484902c4&amp;x-cos-security-token=677e9f4460011955f6fb7b236ef0f5656c17ab5810001" alt=""></figure>
<figure data-type="image" tabindex="10"><img src="https://picture-1257139399.cos.ap-chengdu.myqcloud.com/picture/%E5%AF%BC%E5%90%91%E6%BB%A4%E6%B3%A2.png?q-sign-algorithm=sha1&amp;q-ak=AKID37IJFfEoSU66JdlDLfN0cDSxBSXta3ji&amp;q-sign-time=1550408753;1550410553&amp;q-key-time=1550408753;1550410553&amp;q-header-list=&amp;q-url-param-list=&amp;q-signature=ba4a3e100e809e9172cc96b074d1de60694a5b11&amp;x-cos-security-token=68d664244f467379c4b46fe0b7d4bf36217eb6ec10001" alt=""></figure>
<h3 id="恢复图像">恢复图像</h3>
<p>这里需要注意的是，参与运算的图像有浮点型和整数型，整数型要归一化才可以和浮点型运算。</p>
<pre><code class="language-c++">int RecoverPicture(Mat* scr_img, Mat* t_img, Mat* recover_img, int* A)
{
	float average = (float)(A[0] + A[1] + A[2]) / 3;
	cout &lt;&lt;&quot;A&quot;&lt;&lt; average/255 &lt;&lt; endl;
	if (average &gt; 220)
		average = 220;
	for (int row = 0; row &lt; scr_img-&gt;rows; row++)
	{
		for (int col = 0; col &lt; scr_img-&gt;cols; col++)
		{
			for (int i = 0; i &lt; 3; i++)
			{
				recover_img-&gt;at&lt;Vec3f&gt;(row, col)[i] = ((float)scr_img-&gt;at&lt;Vec3b&gt;(row, col)[i]/255 -average/255) / t_img-&gt;at&lt;float&gt;(row, col) + average /255;
			}
		}
	}
	return 0;
}
</code></pre>
<figure data-type="image" tabindex="11"><img src="https://picture-1257139399.cos.ap-chengdu.myqcloud.com/picture/%E7%BB%93%E6%9E%9C.png?q-sign-algorithm=sha1&amp;q-ak=AKIDrzrQ6phIafnfI4dzhBt5SQEUmycUL2DJ&amp;q-sign-time=1550408693;1550410493&amp;q-key-time=1550408693;1550410493&amp;q-header-list=&amp;q-url-param-list=&amp;q-signature=736df773a1fe4c9fd3d8205e3a6bcb01463ec681&amp;x-cos-security-token=61298ad80dbf09dc974fd71dad7e35887c3b0de410001" alt=""></figure>
<h3 id="不足">不足</h3>
<p>如果图像中，天空区域过亮或者具有大面积天空区域，去雾图片会出现色斑效应，看网上有人指出可能是t0的最小值为to，当大量t趋于一个值时就会出现色斑效应，另外如果大气光值找的不准确，色彩就会失真，这些问题等后期再解决。</p>
<figure data-type="image" tabindex="12"><img src="https://picture-1257139399.cos.ap-chengdu.myqcloud.com/picture/%E7%BC%BA%E7%82%B9.png?q-sign-algorithm=sha1&amp;q-ak=AKIDxNMBTFnufgykmq90SU4g7e1MBtnqJwZu&amp;q-sign-time=1550408467;1550410267&amp;q-key-time=1550408467;1550410267&amp;q-header-list=&amp;q-url-param-list=&amp;q-signature=478b17abe4434ed95de30ebd38a47b134efc27b3&amp;x-cos-security-token=28d672ceeaaaa65acdb159619f153aeff06efa4f10001" alt=""></figure>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[搜索技巧]]></title>
        <id>https://bixiaopeng0.github.io/post/sou-suo-ji-qiao</id>
        <link href="https://bixiaopeng0.github.io/post/sou-suo-ji-qiao">
        </link>
        <updated>2018-07-31T08:18:20.000Z</updated>
        <summary type="html"><![CDATA[<h2 id="双引号完全匹配">双引号完全匹配</h2>
<p>​	在关键词上加上双引号，这样搜素引擎只返回和关键词完全符合的搜素结果。</p>
]]></summary>
        <content type="html"><![CDATA[<h2 id="双引号完全匹配">双引号完全匹配</h2>
<p>​	在关键词上加上双引号，这样搜素引擎只返回和关键词完全符合的搜素结果。</p>
<!--more-->
<figure data-type="image" tabindex="1"><img src="https://picture-1257139399.cos.ap-chengdu.myqcloud.com/find/shuang.png" alt=""></figure>
<h2 id="排除关键词">排除关键词</h2>
<p>​	如果搜素的部分内容不是自己想要的结果，可以使用 - 排除制定内容。使用形式：A -B 注意A后面的空格不能去掉，B紧跟减号。</p>
<figure data-type="image" tabindex="2"><img src="https://picture-1257139399.cos.ap-chengdu.myqcloud.com/find/-.png" alt=""></figure>
<h2 id="在指定网站内搜素">在指定网站内搜素</h2>
<p>​	在输入框内输入site:网址 关键字 就在在输入的网址内进行关键字搜素。</p>
<figure data-type="image" tabindex="3"><img src="https://picture-1257139399.cos.ap-chengdu.myqcloud.com/find/site.png" alt=""></figure>
<h2 id="搜素指定格式的文件">搜素指定格式的文件</h2>
<p>​	这个在下载电子书时，还是蛮重要的。在输入框键入filetype:文件格式 输入关键词后，搜素结果只会显示指定格式的文件。另外，用谷歌搜素外网电子书真的是好用。</p>
<figure data-type="image" tabindex="4"><img src="https://picture-1257139399.cos.ap-chengdu.myqcloud.com/find/filetype.png" alt=""></figure>
<figure data-type="image" tabindex="5"><img src="https://picture-1257139399.cos.ap-chengdu.myqcloud.com/find/filetype1.png" alt=""></figure>
<p>另外上述搜素技巧可以组合使用哟。</p>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[2018电赛程序实现---电流信号检测装置]]></title>
        <id>https://bixiaopeng0.github.io/post/2018-dian-sai-cheng-xu-shi-xian-dian-liu-xin-hao-jian-ce-zhuang-zhi</id>
        <link href="https://bixiaopeng0.github.io/post/2018-dian-sai-cheng-xu-shi-xian-dian-liu-xin-hao-jian-ce-zhuang-zhi">
        </link>
        <updated>2018-07-27T08:18:52.000Z</updated>
        <summary type="html"><![CDATA[<p>​	把程序主要分为三个部分，第一个部分是vga显示，第二个部分是ad7106的驱动程序，第三个部分是fft程序</p>
]]></summary>
        <content type="html"><![CDATA[<p>​	把程序主要分为三个部分，第一个部分是vga显示，第二个部分是ad7106的驱动程序，第三个部分是fft程序</p>
<!--more-->
<h2 id="vga显示">vga显示</h2>
<p>​	vga显示这部分是我队友写的，主要功能显示输入信号的频谱和正弦信号的频率，非正弦信号的谐波幅值，其实这个频谱是经过处理之后显示的，如果输入的是单一频率的正弦波，那么只显示一根谱线，假设输入的是谐波，那么只显示谐波对应频率下的谱线，本来是想把所有的频谱给显示出来的，但是由于vga分辨率是1440X900,fft变化点数是8000个点，屏幕分辨率不够，所以当时没有全部显示，显示效果下图所示：<img src="https://picture-1257139399.cos.ap-chengdu.myqcloud.com/picture/IMG_20180727_142329.jpg" alt=""></p>
<h2 id="ad7606驱动">AD7606驱动</h2>
<p>​	我们一开始采用的是ads828这个10bit70M采样率，但是这款ad有两个问题，测量10mv的信号时准确度低，原因在于利用的有效位少了，这个当时我们想了两种解决思路，第一种是100MA<sub>1A的电流直接经过ad处理，10ma</sub>100ma的电流先经过放大，然后再经过ad处理，这样ad的有效数据位就利用了起来。第二种方案是采用精度更高的ad，我们最后是采用的第二种思路。另一个是这个ad828在硬件设计是加入了运算放大器，导致他的输入电压和输出值并不是线性关系，不过这个可以通过软件补偿进行弥补，先测试100组数据，然后再通过线性拟合补偿。</p>
<p>​	ad7606采用的是spi接口，当时是老师帮我们从网上下载了一份verilog程序，结果还挺好用的，节省了大量的调试时间。</p>
<h2 id="fft">FFT</h2>
<p>​	fft分为采样速率模块控制采样率，fifo控制不同的时钟域，fft出来的数据进行转换，最后利用转换出来的数据进行测频率和幅值。</p>
<h3 id="ip核使用注意事项">ip核使用注意事项</h3>
<ol>
<li>ad输入进来的数据并不是实际电压值，如果要转换成幅度，需要将ad的数据量化成电压值。</li>
<li>关于缩放因子：缩放因子最高位是1则将数据向左移动相应的位数，否则向右移动相应的位数。</li>
<li>缩放完数据 x N/2是原始数据的幅值。</li>
<li>ad产生的数据如果有直流分量要减去直流分量。</li>
<li>fft在matlab仿真中采整数个周期出来的结果才正确，但是在实际工程中，采整数个周期对采样率要求比较严格，实际上只要采样的周期数量足够多的话，就算不是整数个周期，最终的误差影响很小。</li>
<li>fft出来的结果不要最后一个点。</li>
</ol>
<h3 id="采样率">采样率</h3>
<p>​	一开始我们是采用的4K采样率和4K个点，这样频率分辨率1Hz，但是经过我们后期分析，频率分辨率是0.5hz，所以后来又把变化点数提高到8k个点，根据F = Fs / N频率分辨率就变成了0.5hz。</p>
<p>​	4k的采样率一开始是直接在ad驱动程序里设置成1/4k的延时，但是这样不准，ad信号出来时都会有一个busy信号的等待，因为这个时间不固定，所以实际出来的频率小于4khz，这样也会导致fft变换完数据的不准确。一开始，fft出来的结果，当频率变高时，幅值误差很大，我当时以为是采样率太低了，最高被测信号是1khz，4k采1khz，一个周期才测四个点，其实4k的采样率也能满足要求，因为我们变化的8K个点，已经可以恢复出原始信号。由于测试的产生的幅值数据不准，老师给我们提供了一个思路，先把ad7606提高到最高采样率200khz，先对被测数据进行过采样，然后用4k采用率进行抽取进行fft变化，如果大于频率大于800hz采用时域求幅值的方法，频率小于则采用频率测量的方法。当我采用过采样然后抽取的方法时，误打误撞，非常惊喜的发现求出来的数据全部都符合题目要求的精度。</p>
<h3 id="fifo控制">FIFO控制</h3>
<p>​	fft变化的时钟是50Mhz，采样时钟是4k，所以中间采样了一个异步fifo进行缓冲。</p>
<h3 id="fft控制">FFT控制</h3>
<p>​	fft里的数据都是有符号位，当时有效数据标志位开始时，对实部和虚部平方相加然后求根，然后通过缩放因子进行移位，对数据除N/2之后，ad数据转化之后就是实际的电压值。</p>
<h3 id="测频">测频</h3>
<p>​	fft设置的是8k个点，因为频谱是对称的，所以只测前4000个点即可。所测的频率的在谱上的幅值最大，所以进行比较大小，即可求出被测信号中能量最大的信号。</p>
<h2 id="电赛学到的知识">电赛学到的知识</h2>
<ol>
<li>用excel统计大量数据，使用公式行计算求误差，进行拟合。</li>
<li>学会了之前fft没有搞懂的地方，也就是ad转换数据部分。</li>
</ol>
<h2 id="经验总结">经验总结</h2>
<ol>
<li>题目下来几天后，也不知道自己在忙啥，感觉最题目还是理解的不到位，最后还是经过老师带着分析之后才搞懂的，搞懂题目才可以做题，要不然理解错了，做的都是无用功。</li>
<li>从网上传出一份评分表，但是老师不一定完全按照评分表来评测数据。</li>
<li>好几个组做一道题目，方案不要完全一样，起码显示可以改一下，一个用vga，另一个用led显示屏或者数码管。否则老师可能会压你分数。</li>
<li>如果对评测要求不确定，可以准备两套方案，通过按键调整。</li>
</ol>
<p>​</p>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[markdown语法]]></title>
        <id>https://bixiaopeng0.github.io/post/markdown-yu-fa</id>
        <link href="https://bixiaopeng0.github.io/post/markdown-yu-fa">
        </link>
        <updated>2018-04-20T08:39:15.000Z</updated>
        <summary type="html"><![CDATA[<h1 id="序言">序言</h1>
<p>​	前段时间接触过markdown这种轻量级标记语言，感觉用它写文章很棒，可以省去排版的时间，能让人更专注于文档的内容，大大简化在格式上的工作。不过现在用的还不熟，打算以后写博客都用markdown写，学会可以一劳永逸，接下来总结一些常用的markdown语法，供自己以后翻阅。</p>
]]></summary>
        <content type="html"><![CDATA[<h1 id="序言">序言</h1>
<p>​	前段时间接触过markdown这种轻量级标记语言，感觉用它写文章很棒，可以省去排版的时间，能让人更专注于文档的内容，大大简化在格式上的工作。不过现在用的还不熟，打算以后写博客都用markdown写，学会可以一劳永逸，接下来总结一些常用的markdown语法，供自己以后翻阅。</p>
<!--more-->
<h1 id="markdown常用语法">markdown常用语法</h1>
<h2 id="标题">标题</h2>
<h3 id="1用标记">1.用#标记</h3>
<p>在标题开头加上1~6个#，一次代表一级标题、二级标题...六级标题</p>
<figure data-type="image" tabindex="1"><img src="https://picture-1257139399.cos.ap-chengdu.myqcloud.com/2018-07-18_211255.png" alt=""></figure>
<p>效果：</p>
<h1 id="一级标题">一级标题</h1>
<h3 id="三级标题">三级标题</h3>
<h3 id="2-通过文字下方添加和-他们分别表示一级标题和二级标题">2、通过文字下方添加“=”和“-”，他们分别表示一级标题和二级标题</h3>
<p>其中“=”和‘“-”只要大于等于一个就可以，貌似在Typora编辑器里不支持这种标记，我在在线编辑器里试过这种是可行的</p>
<figure data-type="image" tabindex="2"><img src="https://picture-1257139399.cos.ap-chengdu.myqcloud.com/2018-07-18_213256.png" alt=""></figure>
<p>效果：</p>
<figure data-type="image" tabindex="3"><img src="https://picture-1257139399.cos.ap-chengdu.myqcloud.com/2018-07-18_213305.png" alt=""></figure>
<h2 id="斜体">斜体</h2>
<p>如果需要让文字变成斜体，只需要在文字首尾加上*即可</p>
<figure data-type="image" tabindex="4"><img src="https://picture-1257139399.cos.ap-chengdu.myqcloud.com/2018-07-18_213943.png" alt=""></figure>
<p>效果：</p>
<p><em>我是一个斜体</em></p>
<h2 id="粗体">粗体</h2>
<p>如果需要让文字编程粗体，只需要在文字首尾加上**即可</p>
<figure data-type="image" tabindex="5"><img src="https://picture-1257139399.cos.ap-chengdu.myqcloud.com/2018-07-18_214519.png" alt=""></figure>
<p>效果：</p>
<p><strong>加粗的文字</strong></p>
<h2 id="删除线">删除线</h2>
<p>如果需要让文字的中间加上删除线</p>
<figure data-type="image" tabindex="6"><img src="https://picture-1257139399.cos.ap-chengdu.myqcloud.com/2018-07-18_215346.png" alt=""></figure>
<p>效果：</p>
<p><s>删除线</s></p>
<h2 id="换行">换行</h2>
<p>第一行第二行</p>
<h2 id="列表">列表</h2>
<h3 id="无序列表">无序列表</h3>
<p>在文字开头添加*，+，-，其中一种，并且需要和文字之间添加空格</p>
<figure data-type="image" tabindex="7"><img src="https://picture-1257139399.cos.ap-chengdu.myqcloud.com/2018-07-19_074821.png" alt=""></figure>
<p>效果：</p>
<ul>
<li>列表1</li>
<li>列表2</li>
</ul>
<h3 id="有序列表">有序列表</h3>
<p>有序列表使用数字加英文句号表示，并且逗号和文字之间要有空格</p>
<figure data-type="image" tabindex="8"><img src="https://picture-1257139399.cos.ap-chengdu.myqcloud.com/2018-07-19_075609.png" alt=""></figure>
<p>效果：</p>
<ol>
<li>table1</li>
<li>table2</li>
</ol>
<h2 id="分割线">分割线</h2>
<p>在行中使用三个及以上的*、-、_来建立一条分割线。</p>
<figure data-type="image" tabindex="9"><img src="https://picture-1257139399.cos.ap-chengdu.myqcloud.com/2018-07-19_080351.png" alt=""></figure>
<p>效果：</p>
<hr>
<hr>
<hr>
<h2 id="代码块">代码块</h2>
<p>三个```加上使用语言，即可出现代码块</p>
<figure data-type="image" tabindex="10"><img src="https://picture-1257139399.cos.ap-chengdu.myqcloud.com/2018-07-19_080955.png" alt=""></figure>
<p>效果：</p>
<pre><code class="language-c">static void exit_test(const int quit_application) {
    if (global_running_test) {
        longjmp(global_run_test_env, 1);
    } else if (quit_application) {
        exit(-1);
    }
}
</code></pre>
<h2 id="脚注">脚注</h2>
<p>平时写文章时如果涉及到读者陌生的词汇，需要用脚注来解释。</p>
<figure data-type="image" tabindex="11"><img src="https://picture-1257139399.cos.ap-chengdu.myqcloud.com/2018-07-19_081903.png" alt=""></figure>
<p>效果：</p>
<p><sup class="footnote-ref"><a href="#fn1" id="fnref1">[1]</a></sup>脚注1</p>
<h2 id="链接">链接</h2>
<p>[]里是描述文本，()里是链接地址</p>
<figure data-type="image" tabindex="12"><img src="https://picture-1257139399.cos.ap-chengdu.myqcloud.com/2018-07-19_083006.png" alt=""></figure>
<p>效果：</p>
<p>this is <a href="www.baidu.com">baidu</a> website.</p>
<h2 id="图片">图片</h2>
<p>图片比链接前面多了一个！,[]里放的文字是用来描述的图片的关键词，可以不加。另外图片链接的话，一般是现把图片传到图床上，然后复制链接，国内比较有名的网站七牛云。</p>
<figure data-type="image" tabindex="13"><img src="https://picture-1257139399.cos.ap-chengdu.myqcloud.com/2018-07-19_083727.png" alt=""></figure>
<p>效果</p>
<figure data-type="image" tabindex="14"><img src="https://picture-1257139399.cos.ap-chengdu.myqcloud.com/TIM%E5%9B%BE%E7%89%8720180210001317.jpg" alt=""></figure>
<h2 id="换行-2">换行</h2>
<p>enter 是双换行<br>
shift+enter单换行</p>
<h2 id="结束">结束</h2>
<p>大概就这样了，以后用到新的再补充。</p>
<hr class="footnotes-sep">
<section class="footnotes">
<ol class="footnotes-list">
<li id="fn1" class="footnote-item"><p>解释脚注1 <a href="#fnref1" class="footnote-backref">↩︎</a></p>
</li>
</ol>
</section>
]]></content>
    </entry>
</feed>